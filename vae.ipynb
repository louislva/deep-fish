{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124278550>"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fish_drawings = np.load('data/full_numpy_bitmap_fish.npy').reshape((-1, 28, 28))\n",
    "plt.imshow(fish_drawings[0], interpolation='nearest')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1530, 0.1567, 0.1255, 0.1299, 0.1287, 0.1286, 0.1286, 0.1286,\n",
       "          0.1286, 0.1286, 0.1286, 0.1286, 0.1286, 0.1286, 0.1286, 0.1286,\n",
       "          0.1285, 0.1286, 0.1286, 0.1286, 0.1286, 0.1286, 0.1286, 0.1285,\n",
       "          0.1284, 0.1294, 0.1416, 0.1326],\n",
       "         [0.1426, 0.1308, 0.1242, 0.1329, 0.1327, 0.1328, 0.1327, 0.1327,\n",
       "          0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1327,\n",
       "          0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1327, 0.1329,\n",
       "          0.1303, 0.1264, 0.1512, 0.1580],\n",
       "         [0.1438, 0.1143, 0.1549, 0.1632, 0.1643, 0.1644, 0.1643, 0.1642,\n",
       "          0.1642, 0.1642, 0.1642, 0.1642, 0.1642, 0.1642, 0.1642, 0.1642,\n",
       "          0.1642, 0.1642, 0.1642, 0.1641, 0.1641, 0.1641, 0.1640, 0.1642,\n",
       "          0.1642, 0.1567, 0.1774, 0.2060],\n",
       "         [0.1448, 0.1119, 0.1530, 0.1569, 0.1592, 0.1590, 0.1590, 0.1589,\n",
       "          0.1588, 0.1589, 0.1588, 0.1588, 0.1589, 0.1589, 0.1590, 0.1590,\n",
       "          0.1590, 0.1590, 0.1590, 0.1591, 0.1591, 0.1591, 0.1593, 0.1593,\n",
       "          0.1586, 0.1543, 0.1738, 0.2071],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1577, 0.1630, 0.1626, 0.1627, 0.1626,\n",
       "          0.1625, 0.1625, 0.1625, 0.1625, 0.1626, 0.1626, 0.1627, 0.1628,\n",
       "          0.1627, 0.1627, 0.1627, 0.1627, 0.1627, 0.1628, 0.1630, 0.1631,\n",
       "          0.1625, 0.1570, 0.1776, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1532, 0.1579, 0.1633, 0.1629, 0.1630, 0.1630,\n",
       "          0.1629, 0.1629, 0.1628, 0.1629, 0.1629, 0.1630, 0.1631, 0.1630,\n",
       "          0.1629, 0.1629, 0.1629, 0.1629, 0.1629, 0.1631, 0.1632, 0.1633,\n",
       "          0.1627, 0.1572, 0.1776, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1532, 0.1577, 0.1635, 0.1630, 0.1631, 0.1630,\n",
       "          0.1630, 0.1630, 0.1629, 0.1630, 0.1630, 0.1629, 0.1630, 0.1629,\n",
       "          0.1627, 0.1628, 0.1628, 0.1627, 0.1627, 0.1629, 0.1630, 0.1631,\n",
       "          0.1625, 0.1571, 0.1775, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1532, 0.1578, 0.1637, 0.1632, 0.1633, 0.1633,\n",
       "          0.1632, 0.1632, 0.1630, 0.1631, 0.1631, 0.1631, 0.1630, 0.1629,\n",
       "          0.1627, 0.1628, 0.1628, 0.1627, 0.1627, 0.1628, 0.1629, 0.1631,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1532, 0.1578, 0.1637, 0.1632, 0.1633, 0.1633,\n",
       "          0.1632, 0.1631, 0.1630, 0.1632, 0.1631, 0.1629, 0.1630, 0.1628,\n",
       "          0.1626, 0.1626, 0.1627, 0.1626, 0.1626, 0.1628, 0.1628, 0.1630,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1579, 0.1638, 0.1634, 0.1634, 0.1634,\n",
       "          0.1634, 0.1632, 0.1632, 0.1633, 0.1633, 0.1630, 0.1629, 0.1628,\n",
       "          0.1626, 0.1626, 0.1627, 0.1624, 0.1623, 0.1626, 0.1627, 0.1629,\n",
       "          0.1624, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1580, 0.1639, 0.1634, 0.1634, 0.1633,\n",
       "          0.1633, 0.1631, 0.1632, 0.1632, 0.1631, 0.1629, 0.1628, 0.1627,\n",
       "          0.1626, 0.1626, 0.1626, 0.1624, 0.1623, 0.1626, 0.1627, 0.1629,\n",
       "          0.1624, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1580, 0.1641, 0.1636, 0.1636, 0.1636,\n",
       "          0.1635, 0.1633, 0.1634, 0.1633, 0.1631, 0.1629, 0.1627, 0.1627,\n",
       "          0.1625, 0.1625, 0.1625, 0.1624, 0.1623, 0.1626, 0.1627, 0.1628,\n",
       "          0.1624, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1534, 0.1581, 0.1641, 0.1636, 0.1636, 0.1635,\n",
       "          0.1635, 0.1632, 0.1632, 0.1631, 0.1629, 0.1628, 0.1626, 0.1626,\n",
       "          0.1625, 0.1626, 0.1626, 0.1625, 0.1624, 0.1626, 0.1628, 0.1629,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1534, 0.1580, 0.1639, 0.1634, 0.1634, 0.1633,\n",
       "          0.1632, 0.1631, 0.1630, 0.1630, 0.1626, 0.1626, 0.1624, 0.1625,\n",
       "          0.1625, 0.1625, 0.1627, 0.1625, 0.1625, 0.1626, 0.1628, 0.1629,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1535, 0.1581, 0.1639, 0.1634, 0.1633, 0.1633,\n",
       "          0.1632, 0.1630, 0.1629, 0.1628, 0.1626, 0.1626, 0.1627, 0.1626,\n",
       "          0.1625, 0.1626, 0.1627, 0.1626, 0.1626, 0.1626, 0.1629, 0.1629,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1534, 0.1581, 0.1638, 0.1633, 0.1632, 0.1632,\n",
       "          0.1630, 0.1629, 0.1627, 0.1627, 0.1626, 0.1625, 0.1625, 0.1625,\n",
       "          0.1624, 0.1625, 0.1627, 0.1625, 0.1627, 0.1627, 0.1630, 0.1630,\n",
       "          0.1625, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1534, 0.1580, 0.1637, 0.1632, 0.1633, 0.1632,\n",
       "          0.1631, 0.1630, 0.1628, 0.1628, 0.1627, 0.1627, 0.1625, 0.1625,\n",
       "          0.1625, 0.1625, 0.1627, 0.1625, 0.1627, 0.1628, 0.1631, 0.1630,\n",
       "          0.1626, 0.1571, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1533, 0.1579, 0.1636, 0.1632, 0.1633, 0.1633,\n",
       "          0.1631, 0.1630, 0.1627, 0.1627, 0.1625, 0.1625, 0.1624, 0.1624,\n",
       "          0.1625, 0.1625, 0.1627, 0.1626, 0.1629, 0.1629, 0.1632, 0.1631,\n",
       "          0.1626, 0.1572, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1532, 0.1579, 0.1636, 0.1632, 0.1633, 0.1634,\n",
       "          0.1631, 0.1630, 0.1627, 0.1627, 0.1626, 0.1625, 0.1624, 0.1625,\n",
       "          0.1625, 0.1625, 0.1629, 0.1628, 0.1630, 0.1631, 0.1632, 0.1632,\n",
       "          0.1627, 0.1572, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1132, 0.1533, 0.1580, 0.1637, 0.1633, 0.1634, 0.1634,\n",
       "          0.1631, 0.1631, 0.1628, 0.1629, 0.1628, 0.1628, 0.1628, 0.1629,\n",
       "          0.1630, 0.1630, 0.1632, 0.1629, 0.1631, 0.1632, 0.1633, 0.1632,\n",
       "          0.1627, 0.1572, 0.1775, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1579, 0.1636, 0.1631, 0.1633, 0.1633,\n",
       "          0.1631, 0.1631, 0.1629, 0.1629, 0.1629, 0.1628, 0.1628, 0.1629,\n",
       "          0.1630, 0.1630, 0.1631, 0.1630, 0.1630, 0.1632, 0.1633, 0.1632,\n",
       "          0.1627, 0.1572, 0.1774, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1533, 0.1578, 0.1635, 0.1630, 0.1631, 0.1632,\n",
       "          0.1630, 0.1630, 0.1630, 0.1629, 0.1630, 0.1629, 0.1630, 0.1630,\n",
       "          0.1631, 0.1630, 0.1630, 0.1630, 0.1630, 0.1632, 0.1633, 0.1631,\n",
       "          0.1626, 0.1572, 0.1775, 0.2100],\n",
       "         [0.1443, 0.1131, 0.1531, 0.1577, 0.1635, 0.1630, 0.1632, 0.1632,\n",
       "          0.1630, 0.1630, 0.1630, 0.1630, 0.1630, 0.1630, 0.1630, 0.1631,\n",
       "          0.1631, 0.1631, 0.1631, 0.1631, 0.1632, 0.1632, 0.1635, 0.1635,\n",
       "          0.1629, 0.1575, 0.1776, 0.2101],\n",
       "         [0.1443, 0.1134, 0.1539, 0.1586, 0.1648, 0.1641, 0.1641, 0.1642,\n",
       "          0.1640, 0.1641, 0.1642, 0.1641, 0.1641, 0.1641, 0.1641, 0.1641,\n",
       "          0.1641, 0.1640, 0.1641, 0.1641, 0.1641, 0.1642, 0.1641, 0.1640,\n",
       "          0.1631, 0.1577, 0.1775, 0.2100],\n",
       "         [0.1442, 0.1143, 0.1556, 0.1556, 0.1631, 0.1626, 0.1626, 0.1626,\n",
       "          0.1626, 0.1626, 0.1627, 0.1626, 0.1627, 0.1626, 0.1626, 0.1626,\n",
       "          0.1626, 0.1626, 0.1626, 0.1626, 0.1626, 0.1626, 0.1627, 0.1625,\n",
       "          0.1617, 0.1556, 0.1743, 0.2112],\n",
       "         [0.1414, 0.1063, 0.1470, 0.1425, 0.1486, 0.1483, 0.1484, 0.1484,\n",
       "          0.1483, 0.1483, 0.1484, 0.1483, 0.1484, 0.1483, 0.1483, 0.1483,\n",
       "          0.1483, 0.1483, 0.1484, 0.1484, 0.1483, 0.1484, 0.1483, 0.1484,\n",
       "          0.1502, 0.1490, 0.1688, 0.2098],\n",
       "         [0.1553, 0.1124, 0.1825, 0.1792, 0.1834, 0.1833, 0.1833, 0.1833,\n",
       "          0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833,\n",
       "          0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833, 0.1833,\n",
       "          0.1827, 0.1843, 0.1937, 0.2385],\n",
       "         [0.1675, 0.1483, 0.1934, 0.1944, 0.1974, 0.1973, 0.1972, 0.1972,\n",
       "          0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972,\n",
       "          0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972, 0.1972,\n",
       "          0.1976, 0.1959, 0.1933, 0.2134]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        \n",
    "        # input image -> encoding\n",
    "        self.conv1 = nn.Conv2d(1, 8, (3, 3), padding=(1, 1)) # 1x28x28 -> 8x14x14\n",
    "        self.conv2 = nn.Conv2d(8, 8, (3, 3), padding=(1, 1)) # 8x14x14 -> 8x7x7\n",
    "        self.conv3 = nn.Conv2d(8, 16, (3, 3), padding=(1, 1)) # 8x7x7 -> 16x3x3\n",
    "        self.conv4 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1)) # 16x3x3 -> 32x1x1\n",
    "        \n",
    "        self.linear1 = nn.Linear(32, 16)\n",
    "        self.linear2 = nn.Linear(16, 8)\n",
    "        self.linear3 = nn.Linear(8, 4)\n",
    "        \n",
    "        # encoding -> probability distributions\n",
    "        self.linear_mu = nn.Linear(4, 4)\n",
    "        self.linear_log_var = nn.Linear(4, 4)\n",
    "        \n",
    "        # distribution sample -> decoded image\n",
    "        self.uplinear1 = nn.Linear(4, 8)\n",
    "        self.uplinear2 = nn.Linear(8, 16)\n",
    "        self.uplinear3 = nn.Linear(16, 32)\n",
    "        \n",
    "        self.deconv1 = nn.Conv2d(32, 16, (3, 3), padding=(2, 2)) # 1x1x32 -> 4x4x16\n",
    "        self.deconv2 = nn.Conv2d(16, 8, (3, 3), padding=(2, 2)) # 4x4x16 -> 10x10x8\n",
    "        self.deconv3 = nn.Conv2d(8, 8, (3, 3), padding=(2, 2)) # 10x10x8 -> 22x22x8\n",
    "        self.deconv4 = nn.Conv2d(8, 4, (3, 3), padding=(2, 2)) # 22x22x8 -> 24x24x4\n",
    "        self.deconv5 = nn.Conv2d(4, 4, (3, 3), padding=(2, 2)) # 24x24x4 -> 26x26x4\n",
    "        self.deconv6 = nn.Conv2d(4, 1, (3, 3), padding=(2, 2)) # 26x26x4 -> 28x28x4\n",
    "        \n",
    "        # reconstruction loss function parameters\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # make sure it's 2d, that there's one channel, and that it's encoded as batch\n",
    "        x = torch.reshape(x, (-1, 1, 28, 28))\n",
    "        \n",
    "        # reduce with conv & pool; 28x28x1 -> 1x1x32\n",
    "        x = self.conv1(x)\n",
    "        x = nn.MaxPool2d((2, 2))(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = nn.MaxPool2d((2, 2))(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.MaxPool2d((2, 2))(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = nn.MaxPool2d((2, 2))(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        # reduce with fully connected from 2x2x16 (64) -> 16\n",
    "        x = torch.reshape(x, (-1, 32))\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "                \n",
    "        mu = self.linear_mu(x)\n",
    "        log_var = self.linear_log_var(x)\n",
    "\n",
    "        return x, mu, log_var\n",
    "    \n",
    "    def decode(self, x):        \n",
    "        x = self.uplinear1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.uplinear2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.uplinear3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 32, 1, 1))\n",
    "        \n",
    "        x = nn.Upsample(scale_factor=2)(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "           \n",
    "        x = nn.Upsample(scale_factor=2)(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = nn.Upsample(scale_factor=2)(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.deconv4(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.deconv5(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.deconv6(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        return x.reshape((-1, 28, 28))\n",
    "        \n",
    "vae = VariationalAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2182/8384\n",
      "Loss: tensor(4356.8955, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-682-a857655408ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfish_drawings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-682-a857655408ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_var\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-660-2aac359b4282>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# reduce with conv & pool; 28x28x1 -> 1x1x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    158\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                             self.return_indices)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     return torch.max_pool2d(\n\u001b[0m\u001b[1;32m    576\u001b[0m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normal_probability(mean, scale, sample):\n",
    "    normal_dist = torch.distributions.normal.Normal(mean, scale, validate_args=True)\n",
    "    log_prob = normal_dist.log_prob(sample + 0.1)\n",
    "    \n",
    "    return torch.sum(log_prob, dim=(2, 1))\n",
    "\n",
    "def kl_divergence(mu, var, z):\n",
    "    # for z, how much less likely is it that it came from mu = 0 and var = 1 than the current mu and var?\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(var))\n",
    "    q = torch.distributions.Normal(mu, var)\n",
    "    \n",
    "    log_prob_qz = q.log_prob(z)\n",
    "    log_prob_pz = p.log_prob(z)\n",
    "    \n",
    "    kl = log_prob_qz - log_prob_pz\n",
    "    kl = kl.sum(-1)\n",
    "    return kl\n",
    "    \n",
    "\n",
    "def train(model, data):\n",
    "    epoch_count = 8\n",
    "    batch_size = 16\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "    for epoch_index in range(epoch_count):\n",
    "        batch_count = len(data) // batch_size\n",
    "\n",
    "        \n",
    "        for batch_index in range(batch_count):\n",
    "            batch = data[np.random.randint(0, len(data) - 1, (batch_size,))]\n",
    "            x = torch.from_numpy(batch).float()\n",
    "            \n",
    "            _, mu, log_var = vae.encode(x)\n",
    "            var = torch.exp(log_var / 2)\n",
    "            \n",
    "            # create q distribution from our mean and variance that come from a learned relation to encoding\n",
    "            q = torch.distributions.Normal(mu, var)\n",
    "            z = q.rsample()\n",
    "            \n",
    "            # now decode our distribution sample, z; this should reconstruct the x, so this is x hat\n",
    "            x_ = vae.decode(z)\n",
    "            \n",
    "            # probability of x given x_; found by calculating x pixel probability given x_ as mean and exp(vae.log_scale) as variance\n",
    "            reconstruction_loss = normal_probability(x_, torch.exp(vae.log_scale), x) \n",
    "            \n",
    "            # q should approach Normal(0, 1)\n",
    "            kl_divergence_loss = kl_divergence(mu, var, z)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            loss = (kl_divergence_loss - reconstruction_loss).mean()\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(str(batch_index) + '/' + str(batch_count))\n",
    "            print('Loss:', loss)\n",
    "    \n",
    "train(vae, fish_drawings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imagine(model, z):     \n",
    "    y = vae.decode(z).reshape((-1, 28, 28))\n",
    "       \n",
    "    plt.imshow(y.detach().numpy().reshape((28,28)), interpolation='nearest')\n",
    "    \n",
    "def random_imagination(model):     \n",
    "    q = torch.distributions.Normal(torch.zeros((1, 4)), torch.ones((1, 4)))\n",
    "    z = q.sample()\n",
    "\n",
    "    imagine(model, z)\n",
    "    \n",
    "random_imagination(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115833f44a6a4283b0c488de28bf3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='a', max=2.0, min=-2.0, step=0.25), FloatSlider(valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.imagine_with_sliders(a, b, c, d)>"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def imagine_with_sliders(a,b,c,d):\n",
    "    print(a,b,c,d)\n",
    "    imagine(vae, torch.tensor([a,b,c,d]).float())\n",
    "    \n",
    "interact(\n",
    "    imagine_with_sliders,\n",
    "    a=FloatSlider(min=-2, max=2, step=0.25),\n",
    "    b=FloatSlider(min=-2, max=2, step=0.25),\n",
    "    c=FloatSlider(min=-2, max=2, step=0.25),\n",
    "    d=FloatSlider(min=-2, max=2, step=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict, './vae_state_dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
